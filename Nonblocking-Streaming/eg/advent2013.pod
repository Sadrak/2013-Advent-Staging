=head1 Title

Nonblocking and Streaming 

=head1 Overview

Modern versions of L<Catalyst> allow you to take charge of how your
write operations work.  This enables better support for working inside
common event loops such as L<AnyEvent> and <IO::Async> to support
non blocking writes to stream large files.

=head1 Introduction

When L<Catalyst> was ported to run as a native L<PSGI> application,
great pains were taken to make sure we could properly support delayed
and streaming responses.  However only recently was that ability 
properly exposed as L<Catalyst> API.  Let's review how L<Catalyst>
creates a PSGI application and how the response API has changed to
give you this control.

=head1 PSGI Delayed Response and Streaming

You are likely familiar with the basic L<PSGI> 'hello world' application
which goes something like this:

    my $psgi_app = sub {
      my $env = (@_);
      return [ 200,
        [ 'Content-Type' => 'text/plain' ],
        [ 'Hello World!'] ];
    };

If you put the above into a file C<hello-classic.psgi> you could run it under a decent
webserver very easily:

    $ plackup -s Starman bin/hello-classic.psgi

    013/11/27-09:18:04 Starman::Server (type Net::Server::PreFork) starting! pid(69860)
    Resolved [*]:5000 to [0.0.0.0]:5000, IPv4
    Binding to TCP port 5000 on host 0.0.0.0 with IPv4
    Setting gid to "20 20 20 12 61 79 80 81 98 33 100 204 398 399"
    Starman: Accepting connections at http://*:5000/

And now you can use a commandline tool like telnet to hit that server and
execute HTTP requests:

    $ telnet 127.0.0.1 5000
    Trying 127.0.0.1...
    Connected to localhost.
    Escape character is '^]'.
    GET / HTTP/1.1
    Host: www.test.org

    HTTP/1.1 200 OK
    Content-Type: text/plain
    Transfer-Encoding: chunked
    Date: Wed, 27 Nov 2013 22:49:39 GMT
    Connection: keep-alive

    c
    Hello World!
    0

    Connection closed by foreign host.

Take note of the Response header 'Transfer-Encoding: chunked'.  Since L<Starman>
understand HTTP/1.1 it can send chunked responses in the case when you fail to
provide the content length (or, you give an expected length but specify 'chunked'
anyway).  In this case that 'c' that you see is the length in hexadecimal (12
bytes).  That final '0' means there's no more chunks to send.

Now, L<PSGI> offers a few more tricks for the case when you don't want to fully
pregenerate your response.  In this case instead of returning a tuple (Status,
Headers, Body) from your L<PSGI> application, you return a code reference:

    my $psgi_app = sub {
      my $env = shift;
      return sub {
        my $responder = shift;
        my $writer = $responder->(
          [200, [ 'Content-Type' => 'text/plain' ]]);

        $writer->write('Hello');
        $writer->write(' ');
        $writer->write('World');
        $writer->close;
      };
    };

In this case the C<$delayed_response> coderef gets run by the underlying
server, which is required to pass it a second coderef, the C<$responder>.
So basically your application is 'delayed' it doesn't actually run until
the server runs it, which can allow you to take advantage of event loops
like L<AnyEvent> or L<IO::Async> to schedule your response better.  But
its still useful even under a blocking server like L<Starman>.

Lets look a bit more at the code example.  The C<$responder> coderef can
accept the full $Status, \@Headers, \@Body tuple, but a more interesting
use is to give it just the first two elements of the tuple, as we do in
this example application.  In this case the C<$responder> returns the server's
C<$writer> object, which basically is like a filehandle that points at the
client which originated the request (for example a web browser or the
C<lwp-request> commandline application).  This object defines two methods,
C<write> and C<close>.  Each time you call C<write> you stream some data
out to the client.  Generally the connection stays open until you call
C<close>, which means you can use this for long responses or for techniques
like cometd where you keep a connection to the client open for a long time.

<B*Note:> Even thought I say the $writer object can be thought of as a file
handle to the client, the underlying architecture is both more complex and
yet lacking in some of the features we expect from a filehandle.  For
example, in the current PSGI spec, there's no clear and common way to detect
when you can't write due to some issue with the client or network.  Also,
there's nothing stopping the server from buffering your writes before sending
them on to the client, if it is necessary to do so for scheduling purposes.
For example, the server might buffer your write in memory, or to some other
temporary store if needed.

Hopefully someday the plack-cabal can get together on a specification to
expose and clarify some of these important missing bits.

Let's see what this application looks like under telnet.  We start the psgi
application with C<plackup> as in the previous case.

    $ plackup -s Starman bin/hello-delayed.psgi 
    2013/11/27-09:52:15 Starman::Server (type Net::Server::PreFork) starting! pid(69989)
    Resolved [*]:5000 to [0.0.0.0]:5000, IPv4
    Binding to TCP port 5000 on host 0.0.0.0 with IPv4
    Setting gid to "20 20 20 12 61 79 80 81 98 33 100 204 398 399"
    Starman: Accepting connections at http://*:5000/

And hit it with telnet:

    $ telnet 127.0.0.1 5000
    Trying 127.0.0.1...
    Connected to localhost.
    Escape character is '^]'.
    GET / HTTP/1.1
    Host: www.test.org

    HTTP/1.1 200 OK
    Content-Type: text/plain
    Transfer-Encoding: chunked
    Date: Wed, 27 Nov 2013 22:57:11 GMT
    Connection: keep-alive

    5
    Hello
    1
     
    5
    World
    0

    Connection closed by foreign host.

You'll note this time each of the writes is a separate chunk, with a separate
length.  Now, we used the chunked encoding here to help illustrate how each
call to ->write pushes a separate 'chunk', but to be clear, we could have 
used this streaming interface without chunked transfer encoding. For example,
the same code with estimated length in the header:

    my $psgi_app = sub {
      my $env = shift;
      return sub {
        my $responder = shift;
        my $writer = $responder->(
          [200, [ 
            'Content-Type' => 'text/plain',
            'Content-Length' => '11', ]]);

        $writer->write('Hello');
        $writer->write(' ');
        $writer->write('World');
        $writer->close;
      };
    };

And the telent trace:

    $ telnet 127.0.0.1 5000
    Trying 127.0.0.1...
    Connected to localhost.
    Escape character is '^]'.
    GET / HTTP/1.1
    Host: www.test.org

    HTTP/1.1 200 OK
    Content-Type: text/plain
    Content-Length: 11
    Date: Wed, 27 Nov 2013 23:06:33 GMT
    Connection: keep-alive

    Hello WorldConnection closed by foreign host.

so we still streamed the information, just its not as easy to see from looking
at the telent trace, as it is when using chunked transfer encoding.  But its
definitely still streaming, as you can see if you start Starman in DEBUG mode
and examine the debug output for that request:

    >$ STARMAN_DEBUG=1 plackup -s Starman bin/hello-delayed-with-length.psgi 
    2013/11/27-17:09:44 Starman::Server (type Net::Server::PreFork) starting! pid(71887)
    Resolved [*]:5000 to [0.0.0.0]:5000, IPv4
    Binding to TCP port 5000 on host 0.0.0.0 with IPv4
    Setting gid to "20 20 20 12 61 79 80 81 98 33 100 204 398 399"
    Using no serialization
    Starman: Accepting connections at http://*:5000/
    Beginning prefork (5 processes)
    Starting "5" children
    Child Preforked (71888)
    Child Preforked (71889)
    Child Preforked (71890)
    Parent ready for children.
    Child Preforked (71891)
    Child Preforked (71892)

    2013/11/27-17:09:51 CONNECT TCP Peer: "[127.0.0.1]:61580" Local: "[127.0.0.1]:5000"
    [71888] Read 16 bytes: "GET / HTTP/1.1\r\n"
    [71888] Read 20 bytes: "Host: www.test.org\r\n"
    [71888] Read 2 bytes: "\r\n"
    127.0.0.1 - - [27/Nov/2013:17:09:52 -0600] "GET / HTTP/1.1" 200 - "-" "-"
    [71888] Wrote 126 bytes
    [71888] Wrote 5 bytes
    [71888] Wrote 1 byte
    [71888] Wrote 5 bytes
    [71888] Request done
    [71888] Waiting on previous connection for keep-alive request...
    [71888] Closing connection

Here you can see the output very clearly.  Starman sends 126 bytes to the client
(this is the HTTP header information) followed by three writes of 5, 1 and 5 bytes
each.

BTW, if you actually go at run the code you'll actually see the 1 second 'keep
alive' delay at the end of the response.  Basically after the write calls
->close, it spends a bit of time waiting to see if that connection sends a
a request to hold the connection open.  For Starman this is 1 second, but you
can configure it.  This keep alive is a big part of HTTP 1.1 and all HTTP 1.1
connects are considered persistent unless declared otherwise, so that's why
its important for Starman and other HTTP 1.1 servers to have that ability to
timeout if the client isn't doing anything with the persistent connection.

You can actually see this keep alive in action with STARMAN_DEBUG and if
you are speedy with the second request (or bump up the starman keepalive
timeout).  For example:

    $ telnet 127.0.0.1 5000
    Trying 127.0.0.1...
    Connected to localhost.
    Escape character is '^]'.
    GET / HTTP/1.1              (first request which initiates the connection)
    Host: www.test.org

    HTTP/1.1 200 OK
    Content-Type: text/plain
    Content-Length: 11
    Date: Wed, 27 Nov 2013 23:20:13 GMT
    Connection: keep-alive

    Hello World
    
    GET / HTTP/1.1              (second request over the same connection)
    Host: www.test.org

    HTTP/1.1 200 OK
    Content-Type: text/plain
    Content-Length: 11
    Date: Wed, 27 Nov 2013 23:20:14 GMT
    Connection: keep-alive

    Hello World
    
    Connection closed by foreign host.

Here's the L<Starman> debugging output.

    2013/11/27-17:20:12 CONNECT TCP Peer: "[127.0.0.1]:61630" Local: "[127.0.0.1]:5000"
    [71889] Read 16 bytes: "GET / HTTP/1.1\r\n"
    [71889] Read 20 bytes: "Host: www.test.org\r\n"
    [71889] Read 2 bytes: "\r\n"
    127.0.0.1 - - [27/Nov/2013:17:20:13 -0600] "GET / HTTP/1.1" 200 - "-" "-"
    [71889] Wrote 126 bytes
    [71889] Wrote 5 bytes
    [71889] Wrote 1 byte
    [71889] Wrote 5 bytes
    [71889] Request done
    [71889] Waiting on previous connection for keep-alive request...
    [71889] Read 36 bytes: "GET / HTTP/1.1\r\nHost: www.test.org\r\n"
    [71889] Read 2 bytes: "\r\n"
    127.0.0.1 - - [27/Nov/2013:17:20:14 -0600] "GET / HTTP/1.1" 200 - "-" "-"
    [71889] Wrote 126 bytes
    [71889] Wrote 5 bytes
    [71889] Wrote 1 byte
    [71889] Wrote 5 bytes
    [71889] Request done
    [71889] Waiting on previous connection for keep-alive request...
    [71889] Closing connection

So you can see one connection, two requests.  This keep alive forms the basis
of many long polling and similar techniques (like cometd) for keeping a persistent
connection between the client and the server, for the purposes of enabling speedy
(semi - realtime) type interfaces.  Although of course since L<Starman> is a forked,
blocking server, you'd be limited to the number of persistent connections by the
number of forked children (in L<Starman> the default is 5).  If you want to scale
such a thing, you probably need to switch to a non blocking server, which we will
discuss later.

Again, keep alive and chunked responses are separate matters, just it makes it a
bit easier to visually see what is going on.  Some reasons for streaming
(in my mind) are for when you have very large responses that you'd rather not
buffer in memory or to a temp file, but would prefer to send in bits (sort of like
when you have a very large SQL query and you use a cursor to iterate row by row
rather than load all the rows into an array).  On the other hand, chunked encoding
is useful when you don't know upfront the length you are sending (such as when
getting the length is computationally expensive, or can't be known initially, or
when the intention is to send infinitely.  The two concepts overlap in use case
but the technology themselves is separate.

=head1 The Catalyst PSGI application

So you've probably seen the following example of using L<Catalyst> as a L<PSGI>
application:

    use MyCatalystApp;
    my $app = MyCatalystApp->psgi_app;

And if that file is called C<mycatalystapp.psgi> you can run it under L<Starman>
exactly like the simple examples above:

    $ plackup -s Starman mycatalystapp.psgi

But what is going on under the hood with L<Catalyst>?  What kind of L<PSGI>
application is it?  As it turns out, the actual L<PSGI> coderef follows the
delayed and streaming model.  You can see the full coder over here:

L<https://metacpan.org/source/JJNAPIORK/Catalyst-Runtime-5.90051/lib/Catalyst/Engine.pm#L702>

But here's a snip for discussion, which is from L<Catalyst::Engine>

    sub build_psgi_app {
        my ($self, $app) = @_;
     
        return sub {
            my ($env) = @_;
     
            return sub {
                my ($respond) = @_;
                confess("Did not get a response callback for writer, cannot continue") unless $respond;
                $app->handle_request(env => $env, response_cb => $respond);
            };
        };
    }

In this case <$app> is you L<Catalyst> application (not to be confused with the
context, which munges together your application along with request and response
information).  $app at this point is fully initialized, and all the models, views
and controllers are loaded up.

So what happens is that L<Catalyst> returns to the server a L<PSGI> app in the delayed
response form.  Right here in this method we are not building any responses, we are just
grabbing the PSGI env and the $responder and sending that off to the ->handle_request
method of the main $app.

You can refer to the handle_request method here, btw:

L<https://metacpan.org/source/JJNAPIORK/Catalyst-Runtime-5.90051/lib/Catalyst.pm#L2019>

This is a longer method so we won't snip the full code, but the important thing
to note is that it is this method that prepares the $context, dispatches the
request, and then calls finalize to serve up the response.  The finalize method, you 
can see here:

L<https://metacpan.org/source/JJNAPIORK/Catalyst-Runtime-5.90051/lib/Catalyst.pm#L1830>

And that finalizes the response and also does some housecleaning around stats and logs.
It finalized the response by calling a method called (oddly enough :) ) C<finalize_body>.
BTW, it finalized the headers first, as you might expect.

L<Catalyst> plays a bit of a game here, since C<finalize_body> on Catalyst.pm just
delegates the work to the same named method in L<Catalyst::Engine>.  This might just be
a holdover from the pre PSGI days, but that's what it does now.  So to see the real
guts of how the body gets finalized you need to look over here:

L<https://metacpan.org/source/JJNAPIORK/Catalyst-Runtime-5.90051/lib/Catalyst/Engine.pm#L69>

Now that is a method worth snipping and discussing:

    sub finalize_body {
        my ( $self, $c ) = @_;
        return if $c->response->_has_write_fh;
     
        my $body = $c->response->body;
        no warnings 'uninitialized';
        if ( blessed($body) && $body->can('read') or ref($body) eq 'GLOB' ) {
            my $got;
            do {
                $got = read $body, my ($buffer), $CHUNKSIZE;
                $got = 0 unless $self->write( $c, $buffer );  # same as $c->response->write($body)
            } while $got > 0;
     
            close $body;
        }
        else {
            $self->write( $c, $body );  # same as $c->response->write($body)
        }
     
        my $res = $c->response;
        $res->_writer->close;
        $res->_clear_writer;
     
        return;
    }

Ok, breaking it down.  $res->_writer is the $writer object you get when
you call $responder with just the status and headers.  Remember, at this
point L<Catalyst> has already finalized the headers, so its safe to use them.
If you go look at the C<write> method in L<Catalyst::Response> you'll see what
I mean here.

Ok so, when you give $c->response->body a string, that string gets written all
at once, and if you give it a filehandle, it calls ->read on that in blocks
of size $CHUNKSIZE (which is a global variable you can monkey patch to a higher
or lower value, btw.).  So that's how L<Catalyst> can handle streaming of your
filehandles.

Additionally, L<Catalyst> has long supported the ability to stream writes 
programatically in your Controllers via the L<Catalyst::Response/res-write-data>
method.  This would allow you to stream a response in bits, as you have them.
For example you could do:

    sub myaction :Local {
      my ($self, $c) = @_;
      $c->res->write("Hello");
      $c->res->write(" ");
      $c->res->write("World");
    }

And that would work nearly identically to the much similar example L<PSGI>
application we've already looked at.

L<Catalyst> basically has to adapt its response to fit the L<PSGI>
expectation since its been around a lot longer than L<PSGI> so it has its own
API we need to maintain for backward compatibility reasons.

So between calling the C<write> method on L<Catalyst::Response> and setting
the response body to a filehandle (or filehandle like, for example this could
be an in memory filehandle or an object that presented the filehandle API)
L<Catalyst> leverages a lot of what you can do with a L<PSGI> delayed and/or
streaming application.

So, what is that "return if $c->response->_has_write_fh;" right at the very top
of the method all about?  In order to understand that we need to step back
and think about running your L<PSGI> under an event loop and using nonblocking,
asynchronous I/O

=head1 Nonblocking

Sometimes when you are streaming data you encounter a process that takes a
long time.  For example, you page requires several long running SQL queries
to build.  Or you have a simple case where you want to stream something
infinitely.  For example, here's an application that sends the time as a
string followed my a newline once a second, and continues doing so unless
the client breaks the connection:

    my $psgi_app = sub {
      my $env = shift;
      return sub {
        my $responder = shift;
        my $writer = $responder->(
          [200, [ 'Content-Type' => 'text/plain' ]]);

        while(1) {
          $writer->write(scalar localtime);
          sleep 1;
        }

      };
    };

You might do something like this as a heartbeat monitor, or to stream date
infinitely to a client (stuff like log into, server status, anything you
want realtime, or even for gaming applications where you are sending out
information about the location of a character in realtime).

The problem here is that when using a blocking server like L<Starman>
you can never have more infinite connections than you have running forks
(defaults to 5 in the current version of L<Starman>.)   So, if you are
running a huge gaming server that wants to send out continuous information
about the realtime locations of characters and actions what can you do?
You can run lots and lots of servers (expensive).  Or you can change your
architecture to have the clients poll the server at regular intervals to
get updated data.  This is a good option and can scale very well, but it
is not suitable for all use cases.  If you have that specific use case
where you need to maintain a long lived connection to each client, and you
expect 100s or thousands or more clients, what can you do?

Modern operating systems allow for a concept called 'non blocking I/O'
this means that when you initiate I/O, over a socket, or a filehandle,
you don't need to wait until the process is complete, but instead you can
register a callback (basically an anonymous subroutine) that gets 
called when the I/O job is complete.  You can even register callbacks
to handle exceptional events.  This means that instead of having a limited
number of forked processes that can handled one connection each, you can
have many (limited by how your operating system is setup and what resources it
has) connections all at once.  The trick here is that instead of doing
one thing at a time, we have an event loop which manages a schedule of
events.

Lets rewrite the blocking application to run under an evented server:

    use AnyEvent;
    use warnings;
    use strict;

    my $watcher;
    my $timer_model = sub {
      my $writer = shift;
      $watcher = AnyEvent->timer(
        after => 0,
        interval => 1,
        cb => sub {
          $writer->write(scalar localtime ."\n");
        });
    };

    my $psgi_app = sub {
      my $env = shift;
      return sub {
        my $responder = shift;
        my $writer = $responder->(
          [200, [ 'Content-Type' => 'text/plain' ]]);

        $timer_model->($writer);
        
      };
    };

There's a number of popular systems for managing event loops under Perl,
including L<POE>, L<AnyEvent> and L<IO::Async>.  For this example I choose
to use L<AnyEvent>.  Let's break down the application and see what (if anything)
this is buying use.

First off, instead of just creating an infinite loop, we create an anonymous
subroutine to encapsulate the L<AnyEvent> timer model.  If you look carefully
you'll notice this is a closure, which we need in order to make sure the 
C<$watcher> doesn't go out of scope.  C<$watcher> is a sort of pointer to
the event loop, and if it gets undefined by going out of scope, then the timer
itself would get removed from the event loop, and never run.  We could have
just as easily made this a real object, but a closure is simpler for this case.

Then, when the the L<PSGI> application runs and receives a request, it invokes
the closure with the C<$writer> object.  This then starts a timer event, which
every second runs the callback to output the time.

So, how is this better than the shorter an more simple version that runs under
L<Starman>?  This timer is not waiting a second between callback.  It not
blocking the server.  As a result, the L<Twiggy> based server can accept many
more connections than L<Starman> with relatively low overhead in doing so.
Generally you can have hundreds or even thousands of concurrent connections
using this technique and have far lower overhead than if you have L<Starman>
running with 1000 or 2000 forked children.  We can see this in action

    $ TWIGGY_DEBUG=1 plackup bin/time-server-evented.psgi 
    Listening on 0.0.0.0:5000
    Twiggy: Accepting connections at http://0.0.0.0:5000

Now lets open this in telnet

    $ telnet 127.0.0.1 5000
    Trying 127.0.0.1...
    Connected to localhost.
    Escape character is '^]'.
    GET / HTTP/1.0

    HTTP/1.0 200 OK
    Content-Type: text/plain

    Thu Nov 28 18:12:33 2013
    Thu Nov 28 18:12:34 2013
    Thu Nov 28 18:12:35 2013
    Thu Nov 28 18:12:36 2013
    Thu Nov 28 18:12:37 2013
    Thu Nov 28 18:12:38 2013
    Thu Nov 28 18:12:39 2013
    Thu Nov 28 18:12:40 2013
    Thu Nov 28 18:12:41 2013

If you run this yourself (just checkout the repo on Github, links below) you
will note two things.  First of all, L<Twiggy> is not HTTP/1.1 compliant,
so we send a HTTP/1.0 GET request.  So L<Twiggy> isn't supporting chunked
encoding and you'd have to request keep alive if you want it.  Its not a big
deal just something to pay attention to.

Another thing you'd note is that unlike L<Starman>, L<Twiggy> does not have
a timeout on the connection.  This is because with L<Twiggy> the overhead on
the connection is very low, each incoming request does not need to tie up an
entire child fork (like with L<Starman>, or other forking and/or blocking
servers.)  Instead, the server listens for events and when an event comes in
the appropriate callback gets to handle it.  In the background, the event loop
is managing all the events.

The big win here is that the new version of this app can allow many, many
connections all at once, and each connection can listen to the infinite 
stream (unlike with L<Starman> where you are limited to the number of running
workers).

Lets prove that by using Apache C<ab> ( a simple load tester that comes with
the apache webserver).  First, lets change the application to finalize the
connection atfer 5 signals:

    use AnyEvent;
    use warnings;
    use strict;

    my $watcher;
    my $timer_model = sub {
      my $writer = shift;
      my $count = 1;
      $watcher = AnyEvent->timer(
        after => 0,
        interval => 1,
        cb => sub {
          $writer->write(scalar localtime ."\n");
          if(++$count > 5) {
            $writer->close;
            undef $watcher;
          }
        });
    };

    my $psgi_app = sub {
      my $env = shift;
      return sub {
        my $responder = shift;
        my $writer = $responder->(
          [200, [ 'Content-Type' => 'text/plain' ]]);

        $timer_model->($writer);
        
      };
    };

Start this up:

    $ TWIGGY_DEBUG=1 plackup bin/five-times-evented.psgi 
    Listening on 0.0.0.0:5000
    Twiggy: Accepting connections at http://0.0.0.0:5000/

Ok so each request is going to take 4 seconds to finish.  We can see that
via telnet:

    $ telnet 127.0.0.1 5000
    Trying 127.0.0.1...
    Connected to localhost.
    Escape character is '^]'.
    GET / HTTP/1.0

    HTTP/1.0 200 OK
    Content-Type: text/plain

    Thu Nov 28 19:56:15 2013
    Thu Nov 28 19:56:16 2013
    Thu Nov 28 19:56:17 2013
    Thu Nov 28 19:56:18 2013
    Thu Nov 28 19:56:19 2013
    Connection closed by foreign host.

So, if a similar application was running under L<Starman>, with 5 running forks
(the default), and you hit the server with 100 simultaneous requests, you'd 
expect the entire thing to take about 80 seconds (likely more because of network
latency and so forth, but it would never be faster).  What happens if we hit
this with Apache C<ab> with the same concurrent 100 requests?

$ ab -n100 -c100 http://127.0.0.1:5000/
This is ApacheBench, Version 2.3 <$Revision: 655654 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

    Benchmarking 127.0.0.1 (be patient).....done
      
    Server Hostname:        127.0.0.1
    Server Port:            5000

    Document Path:          /
    Document Length:        0 bytes

    Concurrency Level:      100
    Time taken for tests:   4.039 seconds
    Complete requests:      100
    Failed requests:        2
       (Connect: 0, Receive: 0, Length: 2, Exceptions: 0)
    Write errors:           0
    Total transferred:      4650 bytes
    HTML transferred:       150 bytes
    Requests per second:    24.76 [#/sec] (mean)
    Time per request:       4039.357 [ms] (mean)
    Time per request:       40.394 [ms] (mean, across all concurrent requests)
    Transfer rate:          1.12 [Kbytes/sec] received

So the whole thing took a bit over four seconds!  That's the upside of the extra
work of running everything in an evented manner and taking advantage of nonblocking
IO.

B<Note> I just want to point out to make a robust system you need to take care
to monitor and handle error events, since everything is running under one big
process any uncaught errors can crash the server!

B<Note> I also want to point out that although high concurrency is a great trick
to have, concurrency alone isn't the only answer to high scale.  Depending on the
type of application you are building it may or may not have value.

Ok, so now you have the basics of evented and nonblocking IO!  Lets port this
very application to run as a L<Catalyst> application!



=head1 For More Information

More examples of Asynchronous Code: L<

=head1 Summary

Review of what the reader just learned.

=head1 Author

Your Name <your@email.com> | IRC nick

=cut
